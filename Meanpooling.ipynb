{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54908,"status":"ok","timestamp":1761380137399,"user":{"displayName":"아영","userId":"08504876844685829247"},"user_tz":-540},"id":"ljkjYSN4F23V","outputId":"15ec75fe-bd57-42cc-a2a7-0adcc6f6c0c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":15883,"status":"ok","timestamp":1758380065321,"user":{"displayName":"아영","userId":"08504876844685829247"},"user_tz":-540},"id":"NMg_WjsfnjdS","outputId":"0e728187-5ee5-40aa-bca9-4e3ba2b51ee6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n","Collecting transformers\n","  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: transformers, bitsandbytes\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.56.1\n","    Uninstalling transformers-4.56.1:\n","      Successfully uninstalled transformers-4.56.1\n","Successfully installed bitsandbytes-0.47.0 transformers-4.56.2\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"982d72adbc02438f85ea3296d62fd4c7","pip_warning":{"packages":["transformers"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install --upgrade transformers accelerate bitsandbytes safetensors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22878,"status":"ok","timestamp":1758455769914,"user":{"displayName":"아영","userId":"08504876844685829247"},"user_tz":-540},"id":"EKxmrRYOITe2","outputId":"6826d461-1287-485a-c7c8-3d7adf9e2fc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/salaniz/pycocoevalcap\n","  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-2h2es2qh\n","  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-2h2es2qh\n","  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from pycocoevalcap==1.2) (2.0.10)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.0.2)\n","Building wheels for collected packages: pycocoevalcap\n","  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=46caba41d3a2be0e54882a628354756fe4c7c14de955c4a2d29edc63aa0d53d8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9u6kd5qo/wheels/03/ce/0b/3d3fdeecb09b4f4ebcfb3ff28d27a9f5b3c1a7b73897ad122d\n","Successfully built pycocoevalcap\n","Installing collected packages: pycocoevalcap\n","Successfully installed pycocoevalcap-1.2\n"]}],"source":["!pip install git+https://github.com/salaniz/pycocoevalcap"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":203380,"status":"ok","timestamp":1759055185514,"user":{"displayName":"아영","userId":"08504876844685829247"},"user_tz":-540},"id":"jVOihCfN2NBf","outputId":"134972bf-f221-4b5b-bf3a-0dd7f203dc65"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch\n","  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Collecting torchaudio\n","  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n","Collecting filelock (from torch)\n","  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n","Collecting typing-extensions>=4.10.0 (from torch)\n","  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n","Collecting setuptools (from torch)\n","  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n","Collecting sympy>=1.13.3 (from torch)\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Collecting networkx (from torch)\n","  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n","Collecting jinja2 (from torch)\n","  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n","Collecting fsspec (from torch)\n","  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n","  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n","  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n","  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n","  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.4.0 (from torch)\n","  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n","Collecting numpy (from torchvision)\n","  Downloading numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n","  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n","Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n","  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Collecting MarkupSafe>=2.0 (from jinja2->torch)\n","  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n","Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m157.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m136.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m153.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n","Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n","Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n","    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n","  Attempting uninstall: mpmath\n","    Found existing installation: mpmath 1.3.0\n","    Uninstalling mpmath-1.3.0:\n","      Successfully uninstalled mpmath-1.3.0\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.15.0\n","    Uninstalling typing_extensions-4.15.0:\n","      Successfully uninstalled typing_extensions-4.15.0\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.3\n","    Uninstalling sympy-1.13.3:\n","      Successfully uninstalled sympy-1.13.3\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 75.2.0\n","    Uninstalling setuptools-75.2.0:\n","      Successfully uninstalled setuptools-75.2.0\n","  Attempting uninstall: pillow\n","    Found existing installation: pillow 11.3.0\n","    Uninstalling pillow-11.3.0:\n","      Successfully uninstalled pillow-11.3.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.3\n","    Uninstalling nvidia-nccl-cu12-2.27.3:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufile-cu12\n","    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n","    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n","      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.5\n","    Uninstalling networkx-3.5:\n","      Successfully uninstalled networkx-3.5\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.19.1\n","    Uninstalling filelock-3.19.1:\n","      Successfully uninstalled filelock-3.19.1\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.4.0\n","    Uninstalling triton-3.4.0:\n","      Successfully uninstalled triton-3.4.0\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n","    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.6\n","    Uninstalling Jinja2-3.1.6:\n","      Successfully uninstalled Jinja2-3.1.6\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.8.0+cu126\n","    Uninstalling torch-2.8.0+cu126:\n","      Successfully uninstalled torch-2.8.0+cu126\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.23.0+cu126\n","    Uninstalling torchvision-0.23.0+cu126:\n","      Successfully uninstalled torchvision-0.23.0+cu126\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.8.0+cu126\n","    Uninstalling torchaudio-2.8.0+cu126:\n","      Successfully uninstalled torchaudio-2.8.0+cu126\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n","cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pillow-11.3.0 setuptools-80.9.0 sympy-1.14.0 torch-2.8.0 torchaudio-2.8.0 torchvision-0.23.0 triton-3.4.0 typing-extensions-4.15.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"ae8de18e49674ab697fc970318eb74c3","pip_warning":{"packages":["PIL","_distutils_hack","numpy","torchgen"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install --upgrade --force-reinstall torch torchvision torchaudio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2320,"status":"ok","timestamp":1759765606859,"user":{"displayName":"아영","userId":"08504876844685829247"},"user_tz":-540},"id":"9xzIkaoTMe9c","outputId":"1b2fc7e3-b5e0-4c18-ffd2-596a4ad89a2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ captions_test.json 저장 완료: /content/drive/MyDrive/video/MSRVTT/captions_test_1k.json\n","총 비디오 수: 1000\n"]}],"source":["import json\n","from collections import defaultdict\n","from datasets import load_dataset\n","\n","# MSR-VTT test_1k 불러오기\n","ds = load_dataset(\"friedrichor/MSR-VTT\", \"test_1k\")\n","\n","# video_id 기준으로 caption들을 묶기\n","caption_dict = defaultdict(lambda: {\"captions\": []})\n","\n","for item in ds[\"test\"]:\n","    vid = str(item[\"video_id\"])\n","    cap = item[\"caption\"]\n","    caption_dict[vid][\"captions\"].append(cap)\n","\n","# JSON 저장\n","save_path = \"/content/drive/MyDrive/video/MSRVTT/captions_test_1k.json\"\n","with open(save_path, \"w\") as f:\n","    json.dump(caption_dict, f, indent=2)\n","\n","print(f\"✅ captions_test.json 저장 완료: {save_path}\")\n","print(f\"총 비디오 수: {len(caption_dict)}\")\n"]},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","import torch, os, json, time\n","from PIL import Image\n","from tqdm import tqdm\n","from multiprocessing import Pool, cpu_count\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch16\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","model.eval()\n","\n","# 프레임 리사이즈\n","def _resize_one(args):\n","    in_path, out_path, size = args\n","    try:\n","        img = Image.open(in_path).convert(\"RGB\")\n","        img = img.resize(size)\n","        img.save(out_path)\n","    except Exception:\n","        pass\n","\n","def resize_frames_fast(input_dir, output_dir, size=(224, 224)):\n","    os.makedirs(output_dir, exist_ok=True)\n","    tasks = []\n","    for vid in sorted(os.listdir(input_dir)):\n","        in_vid = os.path.join(input_dir, vid)\n","        out_vid = os.path.join(output_dir, vid)\n","        os.makedirs(out_vid, exist_ok=True)\n","        for f in os.listdir(in_vid):\n","            if f.endswith(\".jpg\"):\n","                tasks.append((os.path.join(in_vid, f),\n","                              os.path.join(out_vid, f),\n","                              size))\n","    print(f\"{len(tasks)} frames resizing using {cpu_count()} workers...\")\n","    with Pool(cpu_count()) as p:\n","        list(tqdm(p.imap_unordered(_resize_one, tasks), total=len(tasks)))\n","    print(\"모든 프레임 리사이즈 완료 (224x224).\")\n","\n","# 비디오 임베딩 추출\n","def extract_video_embedding(frame_dir, num_frames=8, batch_size=8):\n","    frame_files = sorted(os.listdir(frame_dir))[:num_frames]\n","    images = [Image.open(os.path.join(frame_dir, f)).convert(\"RGB\") for f in frame_files]\n","\n","    embeds = []\n","    with torch.no_grad():\n","        for i in range(0, len(images), batch_size):\n","            batch = images[i:i+batch_size]\n","            inputs = processor(images=batch, return_tensors=\"pt\", padding=True).to(device)\n","            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","                feats = model.get_image_features(**inputs)\n","            embeds.append(feats)\n","    embeds = torch.cat(embeds, dim=0)\n","    return embeds.mean(dim=0).cpu()\n","\n","# 텍스트 임베딩 추출 (mean pooling)\n","def get_text_embeddings(caption_dict, video_ids):\n","    all_embeds = []\n","    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","        for vid in tqdm(video_ids, desc=\"Extracting text embeddings (mean pooled)\"):\n","            caps = caption_dict[vid][\"captions\"]\n","            inputs = processor(text=caps, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            feats = model.get_text_features(**inputs)\n","            feats = torch.nn.functional.normalize(feats, dim=-1)\n","            mean_feat = feats.mean(dim=0, keepdim=True)\n","            all_embeds.append(mean_feat.cpu())\n","    return torch.cat(all_embeds, dim=0)\n","\n","# Recall@K 계산\n","def recall_at_k(sim, k=1):\n","    ranks = sim.argsort(dim=1, descending=True)\n","    topk = ranks[:, :k]\n","    correct = torch.arange(sim.size(0)).unsqueeze(1)\n","    hits = (topk == correct).any(dim=1).float()\n","    return hits.mean().item()\n","\n","# 전체 실행 파이프라인\n","start_total = time.time()\n","\n","original_root = \"/content/drive/MyDrive/video/Test_1kA_frames8\"\n","resized_root = \"/content/drive/MyDrive/video/Test_1kA_frames8_224\"\n","resize_frames_fast(original_root, resized_root)\n","\n","video_root = resized_root\n","video_ids = sorted(os.listdir(video_root))\n","print(f\"총 비디오 개수: {len(video_ids)}개\")\n","\n","with open(\"/content/drive/MyDrive/video/MSRVTT/captions_test_1k.json\", \"r\") as f:\n","    caption_dict = json.load(f)\n","\n","# 비디오 임베딩 추출\n","video_embeds = []\n","for vid in tqdm(video_ids, desc=\"Extracting video embeddings (8 frames)\"):\n","    try:\n","        video_embeds.append(extract_video_embedding(os.path.join(video_root, vid)))\n","    except Exception as e:\n","        print(f\"{vid} 처리 중 오류 발생: {e}\")\n","video_mat = torch.stack(video_embeds)\n","print(f\"비디오 임베딩 완료 ({len(video_embeds)}개)\")\n","\n","# 텍스트 임베딩 추출\n","text_mat = get_text_embeddings(caption_dict, video_ids)\n","print(f\"텍스트 임베딩 완료 ({len(text_mat)}개)\")\n","\n","# 저장\n","save_dir = \"/content/drive/MyDrive/video/MSRVTT/\"\n","os.makedirs(save_dir, exist_ok=True)\n","torch.save(video_mat, os.path.join(save_dir, \"clip_video_embeds_8f.pt\"))\n","torch.save(text_mat, os.path.join(save_dir, \"clip_text_embeds_8f.pt\"))\n","print(\"임베딩 저장 완료.\")\n","\n","# 유사도 계산 및 리트리벌 평가\n","video_mat = torch.nn.functional.normalize(video_mat.to(torch.float32), dim=-1)\n","text_mat = torch.nn.functional.normalize(text_mat.to(torch.float32), dim=-1)\n","sim = video_mat @ text_mat.T\n","\n","print(\"\\nRetrieval 성능 (8frame)\")\n","for k in [1, 5, 10]:\n","    print(f\"Recall@{k}: {recall_at_k(sim, k):.4f}\")\n","\n","end_total = time.time()\n","print(f\"\\n전체 소요 시간: {(end_total - start_total)/60:.2f}분\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvC7QeS0xSHL","executionInfo":{"status":"ok","timestamp":1762006520109,"user_tz":-540,"elapsed":13,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"bf02173d-40ea-406b-f393-8bb4319e183f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["8000 frames resizing using 12 workers...\n","100%|██████████| 8000/8000 [08:21<00:00, 15.95it/s]\n","모든 프레임 리사이즈 완료 (224x224).\n","총 비디오 개수: 1000개\n","Extracting video embeddings (8 frames): 100%|██████████| 1000/1000 [00:43<00:00, 22.89it/s]\n","비디오 임베딩 완료 (1000개)\n","Extracting text embeddings (mean pooled): 100%|██████████| 1000/1000 [00:10<00:00, 98.19it/s]\n","텍스트 임베딩 완료 (1000개)\n","임베딩 저장 완료.\n","\n","Retrieval 성능 (8frame)\n","Recall@1: 0.2640\n","Recall@5: 0.5100\n","Recall@10: 0.6290\n","\n","전체 소요 시간: 9.2분\n"]}]},{"cell_type":"code","source":["# 모델 로드 (FP16)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch16\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","model.eval()\n","\n","#  병렬 프레임 리사이즈\n","def _resize_one(args):\n","    in_path, out_path, size = args\n","    try:\n","        img = Image.open(in_path).convert(\"RGB\")\n","        img = img.resize(size)\n","        img.save(out_path)\n","    except Exception:\n","        pass\n","\n","def resize_frames_fast(input_dir, output_dir, size=(224, 224)):\n","    os.makedirs(output_dir, exist_ok=True)\n","    tasks = []\n","    for vid in sorted(os.listdir(input_dir)):\n","        in_vid = os.path.join(input_dir, vid)\n","        out_vid = os.path.join(output_dir, vid)\n","        os.makedirs(out_vid, exist_ok=True)\n","        for f in os.listdir(in_vid):\n","            if f.endswith(\".jpg\"):\n","                tasks.append((os.path.join(in_vid, f),\n","                              os.path.join(out_vid, f),\n","                              size))\n","    print(f\"{len(tasks)} frames resizing using {cpu_count()} workers...\")\n","    with Pool(cpu_count()) as p:\n","        list(tqdm(p.imap_unordered(_resize_one, tasks), total=len(tasks)))\n","    print(\" 모든 프레임 리사이즈 완료 (224x224).\")\n","\n","# 16프레임 버전 경로\n","original_root = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA_frames16\"\n","resized_root = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA_frames16_224\"\n","resize_frames_fast(original_root, resized_root)\n","\n","# 비디오 임베딩 추출 (16프레임)\n","def extract_video_embedding(frame_dir, num_frames=16, batch_size=8):\n","    frame_files = sorted(os.listdir(frame_dir))[:num_frames]\n","    images = [Image.open(os.path.join(frame_dir, f)).convert(\"RGB\") for f in frame_files]\n","    embeds = []\n","    with torch.no_grad():\n","        for i in range(0, len(images), batch_size):\n","            batch = images[i:i+batch_size]\n","            inputs = processor(images=batch, return_tensors=\"pt\", padding=True).to(device)\n","            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","                feats = model.get_image_features(**inputs)\n","            embeds.append(feats)\n","    embeds = torch.cat(embeds, dim=0)\n","    return embeds.mean(dim=0).cpu()\n","\n","# 텍스트 임베딩 (mean pooling)\n","def get_text_embeddings(caption_dict, video_ids):\n","    all_embeds = []\n","    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","        for vid in tqdm(video_ids, desc=\"Extracting text embeddings (mean pooled)\"):\n","            caps = caption_dict[vid][\"captions\"]\n","            inputs = processor(text=caps, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            feats = model.get_text_features(**inputs)\n","            feats = torch.nn.functional.normalize(feats, dim=-1)\n","            mean_feat = feats.mean(dim=0, keepdim=True)\n","            all_embeds.append(mean_feat.cpu())\n","    return torch.cat(all_embeds, dim=0)\n","\n","# 비디오 및 캡션 로드\n","video_root = resized_root\n","video_ids = sorted(os.listdir(video_root))\n","\n","with open(\"/content/drive/MyDrive/VIDEO_RETRIEVAL/captions_test_1k.json\", \"r\") as f:\n","    caption_dict = json.load(f)\n","\n","print(f\"총 비디오 개수: {len(video_ids)}개\")\n","\n","# 비디오 임베딩 추출\n","start_time = time.time()\n","video_embeds = []\n","for vid in tqdm(video_ids, desc=\"Extracting video embeddings (16 frames)\"):\n","    path = os.path.join(video_root, vid)\n","    try:\n","        video_embeds.append(extract_video_embedding(path))\n","    except Exception as e:\n","        print(f\"{vid} 처리 중 오류 발생: {e}\")\n","\n","video_mat = torch.stack(video_embeds)\n","print(f\"비디오 임베딩 완료 ({len(video_embeds)}개)\")\n","\n","# 텍스트 임베딩 추출\n","text_mat = get_text_embeddings(caption_dict, video_ids)\n","\n","# 캐시 저장\n","save_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/\"\n","os.makedirs(save_dir, exist_ok=True)\n","torch.save(video_mat, os.path.join(save_dir, \"clip_video_embeds_16f.pt\"))\n","torch.save(text_mat, os.path.join(save_dir, \"clip_text_embeds_16f.pt\"))\n","print(\"임베딩 저장 완료.\")\n","\n","# Recall@K 계산\n","video_mat = torch.nn.functional.normalize(video_mat, dim=-1)\n","text_mat = torch.nn.functional.normalize(text_mat, dim=-1)\n","\n","video_mat = video_mat.to(torch.float32)\n","text_mat = text_mat.to(torch.float32)\n","\n","sim = video_mat @ text_mat.T\n","\n","def recall_at_k(sim, k=1):\n","    ranks = sim.argsort(dim=1, descending=True)\n","    topk = ranks[:, :k]\n","    correct = torch.arange(sim.size(0)).unsqueeze(1)\n","    hits = (topk == correct).any(dim=1).float()\n","    return hits.mean().item()\n","\n","print(\"\\n Retrieval 성능 (16 frames)\")\n","for k in [1,5,10]:\n","    print(f\"Recall@{k}: {recall_at_k(sim, k):.4f}\")\n","\n","end_time = time.time()\n","print(f\"\\n 총 소요 시간: {(end_time - start_time)/60:.2f}분\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RPTrMQSy_rY","executionInfo":{"status":"ok","timestamp":1762006993010,"user_tz":-540,"elapsed":22,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"973782d7-6cad-450f-d0e5-2432418ee7af"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["16000 frames resizing using 2 workers...\n","100%|██████████| 16000/16000 [1:34:15<00:00,  2.83it/s]\n","모든 프레임 리사이즈 완료 (224x224).\n","총 비디오 개수: 1000개\n","Extracting video embeddings (16 frames): 100%|██████████| 1000/1000 [01:26:37<00:00,  5.19s/it]\n","비디오 임베딩 완료 (1000개)\n","Extracting text embeddings (mean pooled): 100%|██████████| 1000/1000 [00:10<00:00, 98.27it/s]\n","텍스트 임베딩 완료 (1000개)\n","임베딩 저장 완료.\n","\n","Retrieval 성능 (16 frames)\n","Recall@1: 0.2730\n","Recall@5: 0.5260\n","Recall@10: 0.6380\n","\n","전체 소요 시간: 97.2분\n"]}]},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","import torch, os\n","from PIL import Image\n","from tqdm import tqdm\n","import json\n","import time\n","from multiprocessing import Pool, cpu_count\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", message=\".*padding.*\")\n","\n","# 모델 로드 (FP16)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch16\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","model.eval()\n","\n","# 병렬 프레임 리사이즈\n","def _resize_one(args):\n","    in_path, out_path, size = args\n","    try:\n","        img = Image.open(in_path).convert(\"RGB\")\n","        img = img.resize(size)\n","        img.save(out_path)\n","    except Exception:\n","        pass\n","\n","def resize_frames_fast(input_dir, output_dir, size=(224, 224)):\n","    os.makedirs(output_dir, exist_ok=True)\n","    tasks = []\n","    for vid in sorted(os.listdir(input_dir)):\n","        in_vid = os.path.join(input_dir, vid)\n","        out_vid = os.path.join(output_dir, vid)\n","        os.makedirs(out_vid, exist_ok=True)\n","        for f in os.listdir(in_vid):\n","            if f.endswith(\".jpg\"):\n","                tasks.append((os.path.join(in_vid, f),\n","                              os.path.join(out_vid, f),\n","                              size))\n","    print(f\" {len(tasks)} frames resizing using {cpu_count()} workers...\")\n","    with Pool(cpu_count()) as p:\n","        list(tqdm(p.imap_unordered(_resize_one, tasks), total=len(tasks)))\n","    print(\"모든 프레임 리사이즈 완료 (224x224)\")\n","\n","# 32프레임 버전 경로\n","original_root = \"/content/drive/MyDrive/video/MSRVTT/Test_1kA_frames32\"\n","resized_root = \"/content/drive/MyDrive/video/MSRVTT/Test_1kA_frames32_224\"\n","resize_frames_fast(original_root, resized_root)\n","\n","# 비디오 임베딩 추출 (32프레임)\n","def extract_video_embedding(frame_dir, num_frames=32, batch_size=8):\n","    frame_files = sorted(os.listdir(frame_dir))[:num_frames]\n","    images = [Image.open(os.path.join(frame_dir, f)).convert(\"RGB\") for f in frame_files]\n","    embeds = []\n","    with torch.no_grad():\n","        for i in range(0, len(images), batch_size):\n","            batch = images[i:i+batch_size]\n","            inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n","            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","                feats = model.get_image_features(**inputs)\n","            embeds.append(feats)\n","    embeds = torch.cat(embeds, dim=0)\n","    return embeds.mean(dim=0).cpu()\n","\n","# 텍스트 임베딩 (mean pooling)\n","def get_text_embeddings(caption_dict, video_ids):\n","    all_embeds = []\n","    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","        for vid in tqdm(video_ids, desc=\"Extracting text embeddings (mean pooled)\"):\n","            caps = caption_dict[vid][\"captions\"]\n","            inputs = processor(text=caps, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            feats = model.get_text_features(**inputs)\n","            feats = torch.nn.functional.normalize(feats, dim=-1)\n","            mean_feat = feats.mean(dim=0, keepdim=True)\n","            all_embeds.append(mean_feat.cpu())\n","    return torch.cat(all_embeds, dim=0)\n","\n","# 비디오 및 캡션 로드\n","video_root = resized_root\n","video_ids = sorted(os.listdir(video_root))\n","\n","with open(\"/content/drive/MyDrive/video/MSRVTT/captions_test_1k.json\", \"r\") as f:\n","    caption_dict = json.load(f)\n","\n","print(f\"총 비디오 개수: {len(video_ids)}개\")\n","\n","# 비디오 임베딩 추출\n","start_time = time.time()\n","video_embeds = []\n","for vid in tqdm(video_ids, desc=\"Extracting video embeddings (32 frames)\"):\n","    path = os.path.join(video_root, vid)\n","    try:\n","        video_embeds.append(extract_video_embedding(path))\n","    except Exception as e:\n","        print(f\" {vid} 처리 중 오류 발생: {e}\")\n","\n","video_mat = torch.stack(video_embeds)\n","print(f\" 비디오 임베딩 완료 ({len(video_embeds)}개)\")\n","\n","# 텍스트 임베딩 추출\n","text_mat = get_text_embeddings(caption_dict, video_ids)\n","\n","# 캐시 저장\n","save_dir = \"/content/drive/MyDrive/video/MSRVTT/\"\n","os.makedirs(save_dir, exist_ok=True)\n","torch.save(video_mat, os.path.join(save_dir, \"clip_video_embeds_32f.pt\"))\n","torch.save(text_mat, os.path.join(save_dir, \"clip_text_embeds_32f.pt\"))\n","\n","# Recall@K 계산\n","video_mat = torch.nn.functional.normalize(video_mat, dim=-1)\n","text_mat = torch.nn.functional.normalize(text_mat, dim=-1)\n","\n","video_mat = video_mat.to(torch.float32)\n","text_mat = text_mat.to(torch.float32)\n","\n","sim = video_mat @ text_mat.T\n","\n","def recall_at_k(sim, k=1):\n","    ranks = sim.argsort(dim=1, descending=True)\n","    topk = ranks[:, :k]\n","    correct = torch.arange(sim.size(0)).unsqueeze(1)\n","    hits = (topk == correct).any(dim=1).float()\n","    return hits.mean().item()\n","\n","print(\"\\n Retrieval 성능 (32 frames)\")\n","for k in [1, 5, 10]:\n","    print(f\"Recall@{k}: {recall_at_k(sim, k):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rybW8r9qzwVh","executionInfo":{"status":"ok","timestamp":1762007125401,"user_tz":-540,"elapsed":52,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"e5b5986a-f5e9-42e7-8af7-69602482b642"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","32000 frames resizing using 12 workers...\n","100%|██████████| 32000/32000 [03:26<00:00, 154.64it/s]\n","모든 프레임 리사이즈 완료 (224x224).\n","총 비디오 개수: 1000개\n","Extracting video embeddings (32 frames): 100%|██████████| 1000/1000 [02:58<00:00,  5.62it/s]\n","비디오 임베딩 완료 (1000개)\n","Extracting text embeddings (mean pooled): 100%|██████████| 1000/1000 [00:11<00:00, 86.13it/s]\n","캐시 저장 완료!\n","\n","Retrieval 성능 (32 frames)\n","Recall@1: 0.2800\n","Recall@5: 0.5220\n","Recall@10: 0.6410\n","\n","총 소요 시간: 3.16분\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1927052,"status":"ok","timestamp":1759852261375,"user":{"displayName":"아영","userId":"08504876844685829247"},"user_tz":-540},"id":"HeNNaOVQsgqP","outputId":"4c2ca88a-f57e-43b4-c9ad-94468abd9772"},"outputs":[{"name":"stderr","output_type":"stream","text":["Extracting random-sampled frames: 100%|██████████| 1000/1000 [32:02<00:00,  1.92s/it]"]},{"name":"stdout","output_type":"stream","text":["✅ 랜덤 샘플링 완료 — 8, 16, 32 프레임 버전 모두 생성됨.\n","폴더 구조 예시: Test_1kA_random16/videoXXXX/frame_0.jpg ...\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","\n","# 원본 비디오 폴더\n","video_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA\"\n","\n","# 저장 경로 (랜덤 샘플링용)\n","save_dir_8  = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA_random8\"\n","save_dir_16 = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA_random16\"\n","save_dir_32 = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA_random32\"\n","\n","for d in [save_dir_8, save_dir_16, save_dir_32]:\n","    os.makedirs(d, exist_ok=True)\n","\n","def random_sample_indices(total_frames, target_frames):\n","    \"\"\"비디오 길이에서 랜덤하게 프레임 인덱스 선택\"\"\"\n","    if total_frames <= target_frames:\n","        return np.arange(total_frames)\n","    return np.sort(np.random.choice(total_frames, target_frames, replace=False))\n","\n","def extract_frames(video_path, save_root, target_frames):\n","    \"\"\"비디오에서 랜덤 프레임 추출\"\"\"\n","    video_name = os.path.splitext(os.path.basename(video_path))[0]\n","    save_path = os.path.join(save_root, video_name)\n","    os.makedirs(save_path, exist_ok=True)\n","\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    indices = random_sample_indices(total_frames, target_frames)\n","\n","    frames = []\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frames.append(frame)\n","    cap.release()\n","\n","    for i, idx in enumerate(indices):\n","        frame = frames[idx]\n","        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        out_path = os.path.join(save_path, f\"frame_{i}.jpg\")\n","        cv2.imwrite(out_path, cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))\n","\n","# 모든 비디오 처리\n","videos = [f for f in os.listdir(video_dir) if f.endswith(('.mp4'))]\n","\n","for v in tqdm(videos, desc=\"Extracting random-sampled frames\"):\n","    video_path = os.path.join(video_dir, v)\n","    extract_frames(video_path, save_dir_8, 8)\n","    extract_frames(video_path, save_dir_16, 16)\n","    extract_frames(video_path, save_dir_32, 32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4345,"status":"ok","timestamp":1759852339903,"user":{"displayName":"아영","userId":"08504876844685829247"},"user_tz":-540},"id":"AAB9IlK0yHhD","outputId":"3befa2d9-c2af-41b2-8aba-adcb06f6e4a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["총 비디오 수: 1000\n","8프레임 폴더 누락: 0개\n","16프레임 폴더 누락: 0개\n","32프레임 폴더 누락: 0개\n","8프레임 개수 오류: 0개\n","16프레임 개수 오류: 0개\n","32프레임 개수 오류: 0개\n","\n"]}],"source":["import os\n","\n","# 랜덤 샘플링 결과 확인 코드\n","video_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA\"\n","\n","frames8_dir  = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA_random8\"\n","frames16_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA_random16\"\n","frames32_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Test_1kA_random32\"\n","\n","# 비디오 파일 목록\n","videos = [os.path.splitext(f)[0] for f in os.listdir(video_dir) if f.endswith(('.mp4'))]\n","\n","missing_8, missing_16, missing_32 = [], [], []\n","wrong_8, wrong_16, wrong_32 = [], [], []\n","\n","for v in videos:\n","    path8  = os.path.join(frames8_dir, v)\n","    path16 = os.path.join(frames16_dir, v)\n","    path32 = os.path.join(frames32_dir, v)\n","\n","    # 8프레임\n","    if not os.path.exists(path8):\n","        missing_8.append(v)\n","    else:\n","        n8 = len([f for f in os.listdir(path8) if f.endswith(\".jpg\")])\n","        if n8 != 8:\n","            wrong_8.append((v, n8))\n","\n","    # 16프레임\n","    if not os.path.exists(path16):\n","        missing_16.append(v)\n","    else:\n","        n16 = len([f for f in os.listdir(path16) if f.endswith(\".jpg\")])\n","        if n16 != 16:\n","            wrong_16.append((v, n16))\n","\n","    # 32프레임\n","    if not os.path.exists(path32):\n","        missing_32.append(v)\n","    else:\n","        n32 = len([f for f in os.listdir(path32) if f.endswith(\".jpg\")])\n","        if n32 != 32:\n","            wrong_32.append((v, n32))\n","\n","# 요약 출력\n","print(f\"총 비디오 수: {len(videos)}\")\n","print(f\"8프레임 폴더 누락: {len(missing_8)}개\")\n","print(f\"16프레임 폴더 누락: {len(missing_16)}개\")\n","print(f\"32프레임 폴더 누락: {len(missing_32)}개\")\n","print(f\"8프레임 개수 오류: {len(wrong_8)}개\")\n","print(f\"16프레임 개수 오류: {len(wrong_16)}개\")\n","print(f\"32프레임 개수 오류: {len(wrong_32)}개\\n\")\n"]},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","import torch, os\n","from PIL import Image\n","from tqdm import tqdm\n","import json\n","import time\n","from multiprocessing import Pool, cpu_count\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", message=\".*padding.*\")\n","\n","# 모델 로드 (FP16)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch16\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","model.eval()\n","\n","# 병렬 프레임 리사이즈\n","def _resize_one(args):\n","    in_path, out_path, size = args\n","    try:\n","        img = Image.open(in_path).convert(\"RGB\")\n","        img = img.resize(size)\n","        img.save(out_path)\n","    except Exception:\n","        pass\n","\n","def resize_frames_fast(input_dir, output_dir, size=(224, 224)):\n","    os.makedirs(output_dir, exist_ok=True)\n","    tasks = []\n","    for vid in sorted(os.listdir(input_dir)):\n","        in_vid = os.path.join(input_dir, vid)\n","        out_vid = os.path.join(output_dir, vid)\n","        os.makedirs(out_vid, exist_ok=True)\n","        for f in os.listdir(in_vid):\n","            if f.endswith(\".jpg\"):\n","                tasks.append((os.path.join(in_vid, f),\n","                              os.path.join(out_vid, f),\n","                              size))\n","    print(f\" {len(tasks)} frames resizing using {cpu_count()} workers...\")\n","    with Pool(cpu_count()) as p:\n","        list(tqdm(p.imap_unordered(_resize_one, tasks), total=len(tasks)))\n","    print(\"모든 프레임 리사이즈 완료 (224x224)\")\n","\n","# 8프레임 랜덤 샘플링 버전 경로\n","base_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL\"\n","original_root = os.path.join(base_dir, \"Test_1kA_random8\")\n","resized_root = os.path.join(base_dir, \"Test_1kA_random8_224\")\n","resize_frames_fast(original_root, resized_root)\n","\n","# 비디오 임베딩 추출 (8프레임)\n","def extract_video_embedding(frame_dir, num_frames=8, batch_size=8):\n","    frame_files = sorted(os.listdir(frame_dir))[:num_frames]\n","    images = [Image.open(os.path.join(frame_dir, f)).convert(\"RGB\") for f in frame_files]\n","    embeds = []\n","    with torch.no_grad():\n","        for i in range(0, len(images), batch_size):\n","            batch = images[i:i+batch_size]\n","            inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n","            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","                feats = model.get_image_features(**inputs)\n","            embeds.append(feats)\n","    embeds = torch.cat(embeds, dim=0)\n","    return embeds.mean(dim=0).cpu()\n","\n","# 텍스트 임베딩 (mean pooling)\n","def get_text_embeddings(caption_dict, video_ids):\n","    all_embeds = []\n","    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","        for vid in tqdm(video_ids, desc=\"Extracting text embeddings (mean pooled)\"):\n","            caps = caption_dict[vid][\"captions\"]\n","            inputs = processor(text=caps, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            feats = model.get_text_features(**inputs)\n","            feats = torch.nn.functional.normalize(feats, dim=-1)\n","            mean_feat = feats.mean(dim=0, keepdim=True)\n","            all_embeds.append(mean_feat.cpu())\n","    return torch.cat(all_embeds, dim=0)\n","\n","# 비디오 및 캡션 로드\n","video_root = resized_root\n","video_ids = sorted(os.listdir(video_root))\n","\n","with open(os.path.join(base_dir, \"captions_test_1k.json\"), \"r\") as f:\n","    caption_dict = json.load(f)\n","\n","print(f\"총 비디오 개수: {len(video_ids)}개\")\n","\n","# 비디오 임베딩 추출\n","start_time = time.time()\n","video_embeds = []\n","for vid in tqdm(video_ids, desc=\"Extracting video embeddings (8 random frames)\"):\n","    path = os.path.join(video_root, vid)\n","    try:\n","        video_embeds.append(extract_video_embedding(path))\n","    except Exception as e:\n","        print(f\"{vid} 처리 중 오류 발생: {e}\")\n","\n","video_mat = torch.stack(video_embeds)\n","print(f\"비디오 임베딩 완료 ({len(video_embeds)}개)\")\n","\n","# 텍스트 임베딩 추출\n","text_mat = get_text_embeddings(caption_dict, video_ids)\n","\n","# 캐시 저장\n","torch.save(video_mat, os.path.join(base_dir, \"clip_video_embeds_random8f.pt\"))\n","torch.save(text_mat, os.path.join(base_dir, \"clip_text_embeds_random8f.pt\"))\n","\n","# Recall@K 계산\n","video_mat = torch.nn.functional.normalize(video_mat, dim=-1)\n","text_mat = torch.nn.functional.normalize(text_mat, dim=-1)\n","\n","video_mat = video_mat.to(torch.float32)\n","text_mat = text_mat.to(torch.float32)\n","\n","sim = video_mat @ text_mat.T\n","\n","def recall_at_k(sim, k=1):\n","    ranks = sim.argsort(dim=1, descending=True)\n","    topk = ranks[:, :k]\n","    correct = torch.arange(sim.size(0)).unsqueeze(1)\n","    hits = (topk == correct).any(dim=1).float()\n","    return hits.mean().item()\n","\n","print(\"\\n Retrieval 성능 (8 random frames)\")\n","for k in [1, 5, 10]:\n","    print(f\"Recall@{k}: {recall_at_k(sim, k):.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jE1ZXCeW3TIu","executionInfo":{"status":"ok","timestamp":1762008081565,"user_tz":-540,"elapsed":33,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"fc454e48-c161-4f9e-b3fe-55b93b1b6c06"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["7504 frames resizing using 2 workers...\n","100%|██████████| 7504/7504 [16:24<00:00,  7.62it/s]\n","모든 프레임 리사이즈 완료 (224x224).\n","총 비디오 개수: 1000개\n","Extracting video embeddings (8 random frames):   0%|          | 0/1000 [00:00<?, ?it/s]\n","/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn(\n","Extracting video embeddings (8 random frames): 100%|██████████| 1000/1000 [55:12<00:00,  3.31s/it]\n","비디오 임베딩 완료 (1000개)\n","Extracting text embeddings (mean pooled): 100%|██████████| 1000/1000 [00:10<00:00, 99.82it/s]\n","텍스트 임베딩 완료 (1000개)\n","\n","Retrieval 성능 (8 random frames)\n","Recall@1: 0.2710\n","Recall@5: 0.5190\n","Recall@10: 0.6280\n","\n","총 소요 시간: 65.4분\n"]}]},{"cell_type":"code","source":["\n","from transformers import CLIPProcessor, CLIPModel\n","import torch, os\n","from PIL import Image\n","from tqdm import tqdm\n","import json\n","import time\n","from multiprocessing import Pool, cpu_count\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", message=\".*padding.*\")\n","\n","# 모델 로드 (FP16)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch16\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","model.eval()\n","\n","# 병렬 프레임 리사이즈\n","def _resize_one(args):\n","    in_path, out_path, size = args\n","    try:\n","        img = Image.open(in_path).convert(\"RGB\")\n","        img = img.resize(size)\n","        img.save(out_path)\n","    except Exception:\n","        pass\n","\n","def resize_frames_fast(input_dir, output_dir, size=(224, 224)):\n","    os.makedirs(output_dir, exist_ok=True)\n","    tasks = []\n","    for vid in sorted(os.listdir(input_dir)):\n","        in_vid = os.path.join(input_dir, vid)\n","        out_vid = os.path.join(output_dir, vid)\n","        os.makedirs(out_vid, exist_ok=True)\n","        for f in os.listdir(in_vid):\n","            if f.endswith(\".jpg\"):\n","                tasks.append((os.path.join(in_vid, f),\n","                              os.path.join(out_vid, f),\n","                              size))\n","    print(f\" {len(tasks)} frames resizing using {cpu_count()} workers...\")\n","    with Pool(cpu_count()) as p:\n","        list(tqdm(p.imap_unordered(_resize_one, tasks), total=len(tasks)))\n","    print(\"모든 프레임 리사이즈 완료 (224x224).\")\n","\n","# 16프레임 랜덤 샘플링 버전 경로\n","base_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL\"\n","original_root = os.path.join(base_dir, \"Test_1kA_random16\")\n","resized_root = os.path.join(base_dir, \"Test_1kA_random16_224\")\n","resize_frames_fast(original_root, resized_root)\n","\n","# 비디오 임베딩 추출 (16프레임)\n","def extract_video_embedding(frame_dir, num_frames=16, batch_size=8):\n","    frame_files = sorted(os.listdir(frame_dir))[:num_frames]\n","    images = [Image.open(os.path.join(frame_dir, f)).convert(\"RGB\") for f in frame_files]\n","    embeds = []\n","    with torch.no_grad():\n","        for i in range(0, len(images), batch_size):\n","            batch = images[i:i+batch_size]\n","            inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n","            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","                feats = model.get_image_features(**inputs)\n","            embeds.append(feats)\n","    embeds = torch.cat(embeds, dim=0)\n","    return embeds.mean(dim=0).cpu()\n","\n","# 텍스트 임베딩 (mean pooling)\n","def get_text_embeddings(caption_dict, video_ids):\n","    all_embeds = []\n","    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","        for vid in tqdm(video_ids, desc=\"Extracting text embeddings (mean pooled)\"):\n","            caps = caption_dict[vid][\"captions\"]\n","            inputs = processor(text=caps, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            feats = model.get_text_features(**inputs)\n","            feats = torch.nn.functional.normalize(feats, dim=-1)\n","            mean_feat = feats.mean(dim=0, keepdim=True)\n","            all_embeds.append(mean_feat.cpu())\n","    return torch.cat(all_embeds, dim=0)\n","\n","# 비디오 및 캡션 로드\n","video_root = resized_root\n","video_ids = sorted(os.listdir(video_root))\n","\n","with open(os.path.join(base_dir, \"captions_test_1k.json\"), \"r\") as f:\n","    caption_dict = json.load(f)\n","\n","print(f\"총 비디오 개수: {len(video_ids)}개\")\n","\n","# 비디오 임베딩 추출\n","start_time = time.time()\n","video_embeds = []\n","for vid in tqdm(video_ids, desc=\"Extracting video embeddings (16 random frames)\"):\n","    path = os.path.join(video_root, vid)\n","    try:\n","        video_embeds.append(extract_video_embedding(path))\n","    except Exception as e:\n","        print(f\"{vid} 처리 중 오류 발생: {e}\")\n","\n","video_mat = torch.stack(video_embeds)\n","print(f\"비디오 임베딩 완료 ({len(video_embeds)}개)\")\n","\n","# 텍스트 임베딩 추출\n","text_mat = get_text_embeddings(caption_dict, video_ids)\n","\n","# 캐시 저장\n","torch.save(video_mat, os.path.join(base_dir, \"clip_video_embeds_random16f.pt\"))\n","torch.save(text_mat, os.path.join(base_dir, \"clip_text_embeds_random16f.pt\"))\n","\n","# Recall@K 계산\n","video_mat = torch.nn.functional.normalize(video_mat, dim=-1)\n","text_mat = torch.nn.functional.normalize(text_mat, dim=-1)\n","\n","video_mat = video_mat.to(torch.float32)\n","text_mat = text_mat.to(torch.float32)\n","\n","sim = video_mat @ text_mat.T\n","\n","def recall_at_k(sim, k=1):\n","    ranks = sim.argsort(dim=1, descending=True)\n","    topk = ranks[:, :k]\n","    correct = torch.arange(sim.size(0)).unsqueeze(1)\n","    hits = (topk == correct).any(dim=1).float()\n","    return hits.mean().item()\n","\n","print(\"\\n Retrieval 성능 (16 random frames)\")\n","for k in [1, 5, 10]:\n","    print(f\"Recall@{k}: {recall_at_k(sim, k):.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQ99N4Bw3sZI","executionInfo":{"status":"ok","timestamp":1762008311353,"user_tz":-540,"elapsed":46,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"1ad7b78e-a789-447a-daf2-dfc216ed4fbc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","16000 frames resizing using 12 workers...\n","100%|██████████| 16000/16000 [01:39<00:00, 160.19it/s]\n","모든 프레임 리사이즈 완료 (224x224).\n","총 비디오 개수: 1000개\n","Extracting video embeddings (16 random frames): 100%|██████████| 1000/1000 [01:31<00:00, 10.93it/s]\n","비디오 임베딩 완료 (1000개)\n","Extracting text embeddings (mean pooled): 100%|██████████| 1000/1000 [00:11<00:00, 85.22it/s]\n","\n","Retrieval 성능 (16 random frames)\n","Recall@1: 0.2860\n","Recall@5: 0.5210\n","Recall@10: 0.6260\n","\n","총 소요 시간: 1.72분\n"]}]},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","import torch, os\n","from PIL import Image\n","from tqdm import tqdm\n","import json\n","import time\n","from multiprocessing import Pool, cpu_count\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", message=\".*padding.*\")\n","\n","# 모델 로드 (FP16)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch16\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","model.eval()\n","\n","# 병렬 프레임 리사이즈\n","def _resize_one(args):\n","    in_path, out_path, size = args\n","    try:\n","        img = Image.open(in_path).convert(\"RGB\")\n","        img = img.resize(size)\n","        img.save(out_path)\n","    except Exception:\n","        pass\n","\n","def resize_frames_fast(input_dir, output_dir, size=(224, 224)):\n","    os.makedirs(output_dir, exist_ok=True)\n","    tasks = []\n","    for vid in sorted(os.listdir(input_dir)):\n","        in_vid = os.path.join(input_dir, vid)\n","        out_vid = os.path.join(output_dir, vid)\n","        os.makedirs(out_vid, exist_ok=True)\n","        for f in os.listdir(in_vid):\n","            if f.endswith(\".jpg\"):\n","                tasks.append((os.path.join(in_vid, f),\n","                              os.path.join(out_vid, f),\n","                              size))\n","    print(f\" {len(tasks)} frames resizing using {cpu_count()} workers...\")\n","    with Pool(cpu_count()) as p:\n","        list(tqdm(p.imap_unordered(_resize_one, tasks), total=len(tasks)))\n","    print(\" 모든 프레임 리사이즈 완료 (224x224).\")\n","\n","# 32프레임 랜덤 샘플링 버전 경로\n","base_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL\"\n","original_root = os.path.join(base_dir, \"Test_1kA_random32\")\n","resized_root = os.path.join(base_dir, \"Test_1kA_random32_224\")\n","resize_frames_fast(original_root, resized_root)\n","\n","# 비디오 임베딩 추출 (32프레임)\n","def extract_video_embedding(frame_dir, num_frames=32, batch_size=8):\n","    frame_files = sorted(os.listdir(frame_dir))[:num_frames]\n","    images = [Image.open(os.path.join(frame_dir, f)).convert(\"RGB\") for f in frame_files]\n","    embeds = []\n","    with torch.no_grad():\n","        for i in range(0, len(images), batch_size):\n","            batch = images[i:i+batch_size]\n","            inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n","            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","                feats = model.get_image_features(**inputs)\n","            embeds.append(feats)\n","    embeds = torch.cat(embeds, dim=0)\n","    return embeds.mean(dim=0).cpu()\n","\n","# 텍스트 임베딩 (mean pooling)\n","def get_text_embeddings(caption_dict, video_ids):\n","    all_embeds = []\n","    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n","        for vid in tqdm(video_ids, desc=\"Extracting text embeddings (mean pooled)\"):\n","            caps = caption_dict[vid][\"captions\"]\n","            inputs = processor(text=caps, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            feats = model.get_text_features(**inputs)\n","            feats = torch.nn.functional.normalize(feats, dim=-1)\n","            mean_feat = feats.mean(dim=0, keepdim=True)\n","            all_embeds.append(mean_feat.cpu())\n","    return torch.cat(all_embeds, dim=0)\n","\n","# 비디오 및 캡션 로드\n","video_root = resized_root\n","video_ids = sorted(os.listdir(video_root))\n","\n","with open(os.path.join(base_dir, \"captions_test_1k.json\"), \"r\") as f:\n","    caption_dict = json.load(f)\n","\n","print(f\"총 비디오 개수: {len(video_ids)}개\")\n","\n","# 비디오 임베딩 추출\n","start_time = time.time()\n","video_embeds = []\n","for vid in tqdm(video_ids, desc=\"Extracting video embeddings (32 random frames)\"):\n","    path = os.path.join(video_root, vid)\n","    try:\n","        video_embeds.append(extract_video_embedding(path))\n","    except Exception as e:\n","        print(f\" {vid} 처리 중 오류 발생: {e}\")\n","\n","video_mat = torch.stack(video_embeds)\n","print(f\" 비디오 임베딩 완료 ({len(video_embeds)}개)\")\n","\n","# 텍스트 임베딩 추출\n","text_mat = get_text_embeddings(caption_dict, video_ids)\n","\n","# 캐시 저장\n","torch.save(video_mat, os.path.join(base_dir, \"clip_video_embeds_random32f.pt\"))\n","torch.save(text_mat, os.path.join(base_dir, \"clip_text_embeds_random32f.pt\"))\n","\n","# Recall@K 계산\n","video_mat = torch.nn.functional.normalize(video_mat, dim=-1)\n","text_mat = torch.nn.functional.normalize(text_mat, dim=-1)\n","\n","video_mat = video_mat.to(torch.float32)\n","text_mat = text_mat.to(torch.float32)\n","\n","sim = video_mat @ text_mat.T\n","\n","def recall_at_k(sim, k=1):\n","    ranks = sim.argsort(dim=1, descending=True)\n","    topk = ranks[:, :k]\n","    correct = torch.arange(sim.size(0)).unsqueeze(1)\n","    hits = (topk == correct).any(dim=1).float()\n","    return hits.mean().item()\n","\n","print(\"\\n Retrieval 성능 (32 random frames)\")\n","for k in [1, 5, 10]:\n","    print(f\"Recall@{k}: {recall_at_k(sim, k):.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOeRh9uS4j0y","executionInfo":{"status":"ok","timestamp":1762008386130,"user_tz":-540,"elapsed":15,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"458990da-630b-440d-b93e-c1540934562b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","32000 frames resizing using 12 workers...\n","100%|██████████| 32000/32000 [03:28<00:00, 153.68it/s]\n","모든 프레임 리사이즈 완료 (224x224).\n","총 비디오 개수: 1000개\n","Extracting video embeddings (32 random frames): 100%|██████████| 1000/1000 [03:00<00:00,  5.56it/s]\n","비디오 임베딩 완료 (1000개)\n","Extracting text embeddings (mean pooled): 100%|██████████| 1000/1000 [00:11<00:00, 87.02it/s]\n","\n","Retrieval 성능 (32 random frames)\n","Recall@1: 0.2710\n","Recall@5: 0.5230\n","Recall@10: 0.6400\n","\n","총 소요 시간: 3.19분\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191,"status":"ok","timestamp":1759827734254,"user":{"displayName":"아영","userId":"08504876844685829247"},"user_tz":-540},"id":"SsjiPbpW2JI-","outputId":"d10a49de-cd31-4313-fcd8-b6995424fd55"},"outputs":[{"name":"stderr","output_type":"stream","text":["Evaluating experiments: 100%|██████████| 6/6 [00:00<00:00, 35.05it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","📊 Retrieval Evaluation Summary (Recall@K + MedR + MnR)\n","\n","   Setting   R@1   R@5  R@10  MedR  MnR\n"," Uniform_8 0.264 0.510 0.629     4 34.8\n","Uniform_16 0.273 0.526 0.638     4 34.6\n","Uniform_32 0.280 0.522 0.641     4 34.3\n","  Random_8 0.271 0.519 0.628     4 36.2\n"," Random_16 0.286 0.521 0.626     4 36.2\n"," Random_32 0.271 0.523 0.640     4 34.6\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import torch\n","import os\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# 실험 파일 목록\n","experiments = {\n","    \"Uniform_8\":  (\"clip_video_embeds_8f.pt\",  \"clip_text_embeds_8f.pt\"),\n","    \"Uniform_16\": (\"clip_video_embeds_16f.pt\", \"clip_text_embeds_16f.pt\"),\n","    \"Uniform_32\": (\"clip_video_embeds_32f.pt\", \"clip_text_embeds_32f.pt\"),\n","    \"Random_8\":   (\"clip_video_embeds_random8f.pt\",  \"clip_text_embeds_random8f.pt\"),\n","    \"Random_16\":  (\"clip_video_embeds_random16f.pt\", \"clip_text_embeds_random16f.pt\"),\n","    \"Random_32\":  (\"clip_video_embeds_random32f.pt\", \"clip_text_embeds_random32f.pt\"),\n","}\n","\n","search_dirs = [\n","    \"/content/drive/MyDrive/VIDEO_RETRIEVAL\",\n","    \"/content/drive/MyDrive/video/MSRVTT\"\n","]\n","\n","# 지표 계산 함수\n","def compute_metrics(video_mat, text_mat):\n","    video_mat = torch.nn.functional.normalize(video_mat, dim=-1)\n","    text_mat = torch.nn.functional.normalize(text_mat, dim=-1)\n","    sim = video_mat @ text_mat.T\n","\n","    ranks = sim.argsort(dim=1, descending=True)\n","    correct = torch.arange(sim.size(0)).unsqueeze(1)\n","    pos = (ranks == correct).nonzero(as_tuple=False)[:, 1]\n","\n","    def recall_at_k(k):\n","        topk = ranks[:, :k]\n","        hits = (topk == correct).any(dim=1).float()\n","        return hits.mean().item()\n","\n","    R1, R5, R10 = recall_at_k(1), recall_at_k(5), recall_at_k(10)\n","    MedR = pos.median().item()\n","    MnR = pos.float().mean().item()\n","    return R1, R5, R10, MedR, MnR\n","\n","# 파일 탐색 + 결과 계산\n","results = []\n","\n","for name, (vfile, tfile) in tqdm(experiments.items(), desc=\"Evaluating experiments\"):\n","    vpath, tpath = None, None\n","    for d in search_dirs:\n","        if vpath is None and os.path.exists(os.path.join(d, vfile)):\n","            vpath = os.path.join(d, vfile)\n","        if tpath is None and os.path.exists(os.path.join(d, tfile)):\n","            tpath = os.path.join(d, tfile)\n","\n","    if vpath is None or tpath is None:\n","        print(f\"{name}: 파일을 두 경로 어디에서도 찾을 수 없음.\")\n","        continue\n","\n","    video_mat = torch.load(vpath, map_location=\"cpu\").to(torch.float32)\n","    text_mat = torch.load(tpath, map_location=\"cpu\").to(torch.float32)\n","\n","    R1, R5, R10, MedR, MnR = compute_metrics(video_mat, text_mat)\n","    results.append({\n","        \"Setting\": name,\n","        \"R@1\": round(R1, 4),\n","        \"R@5\": round(R5, 4),\n","        \"R@10\": round(R10, 4),\n","        \"MedR\": round(MedR, 1),\n","        \"MnR\": round(MnR, 1)\n","    })\n","\n","order = [\"Uniform_8\", \"Uniform_16\", \"Uniform_32\", \"Random_8\", \"Random_16\", \"Random_32\"]\n","df = pd.DataFrame(results)\n","df[\"Setting\"] = pd.Categorical(df[\"Setting\"], categories=order, ordered=True)\n","df = df.sort_values(\"Setting\")\n","\n","# 결과표 출력\n","print(\"\\n Retrieval Evaluation Summary (Recall@K + MedR + MnR)\\n\")\n","print(df.to_string(index=False))\n"]},{"cell_type":"code","source":["!pip install av tqdm > /dev/null\n","\n","import av\n","import os\n","import numpy as np\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from tqdm import tqdm\n","import cv2\n","\n","train_videos_dir = \"/content/drive/MyDrive/video/MSRVTT/TrainVideo\"\n","val_videos_dir   = \"/content/drive/MyDrive/video/MSRVTT/ValVideo\"\n","\n","train_frames_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Trainframes16\"\n","val_frames_dir   = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/Valframes16\"\n","\n","os.makedirs(train_frames_dir, exist_ok=True)\n","os.makedirs(val_frames_dir, exist_ok=True)\n","\n","def extract_uniform_frames_fast(video_path, save_dir, num_frames=16):\n","    try:\n","        container = av.open(video_path)\n","        stream = container.streams.video[0]\n","        total_frames = stream.frames\n","        if not total_frames or total_frames < num_frames:\n","            return False\n","\n","        # 균등 인덱스 계산\n","        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        frames = []\n","        for i, frame in enumerate(container.decode(stream)):\n","            if i > indices[-1]:\n","                break\n","            if i in indices:\n","                img = frame.to_ndarray(format=\"bgr24\")\n","                out_path = os.path.join(save_dir, f\"{len(frames):03d}.jpg\")\n","                cv2.imwrite(out_path, img)\n","                frames.append(out_path)\n","        container.close()\n","        return True\n","    except Exception as e:\n","        return False\n","\n","def process_videos_parallel(input_dir, output_dir, num_frames=16, max_workers=8):\n","    video_files = [f for f in os.listdir(input_dir)\n","                   if f.lower().endswith((\".mp4\"))]\n","    print(f\"총 {len(video_files)}개의 비디오 처리 중... (병렬 {max_workers} threads)\")\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = []\n","        for vfile in video_files:\n","            video_path = os.path.join(input_dir, vfile)\n","            vid_name = os.path.splitext(vfile)[0]\n","            save_dir = os.path.join(output_dir, vid_name)\n","            futures.append(executor.submit(extract_uniform_frames_fast,\n","                                           video_path, save_dir, num_frames))\n","\n","        for f in tqdm(as_completed(futures), total=len(futures)):\n","            _ = f.result()\n","\n","\n","print(\" Train 세트 16프레임 Uniform Sampling 시작\")\n","process_videos_parallel(train_videos_dir, train_frames_dir, num_frames=16, max_workers=8)\n","\n","print(\"\\n Validation 세트 16프레임 Uniform Sampling 시작\")\n","process_videos_parallel(val_videos_dir, val_frames_dir, num_frames=16, max_workers=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YbCmNLAdLzth","executionInfo":{"status":"ok","timestamp":1762013471974,"user_tz":-540,"elapsed":59,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"9645a374-7017-4a16-d635-d6e8a2ca7586"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Train 세트 16프레임 Uniform Sampling 시작\n","총 6513개의 비디오 처리 중... (병렬 8 threads)\n","100%|██████████| 6513/6513 [20:46<00:00,  5.23it/s]\n","\n","Validation 세트 16프레임 Uniform Sampling 시작\n","총 497개의 비디오 처리 중... (병렬 8 threads)\n","100%|██████████| 497/497 [01:02<00:00,  8.01it/s]\n"]}]},{"cell_type":"code","source":["import os, shutil, random, numpy as np, av, cv2\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from tqdm import tqdm\n","\n","random.seed(42); np.random.seed(42)\n","\n","\n","SRC = {\n","    \"Train\": \"/content/drive/MyDrive/video/MSRVTT/TrainVideo\",\n","    \"Val\":   \"/content/drive/MyDrive/video/MSRVTT/ValVideo\",\n","}\n","TMP = {k: f\"/content/MSRVTT_{k}_TMP\" for k in SRC}\n","\n","for k in SRC:\n","    if not os.path.exists(TMP[k]):\n","        shutil.copytree(SRC[k], TMP[k], dirs_exist_ok=True)\n","\n","DST_LOCAL = {\n","    \"Train\": {8:\"/content/Trainframes_random8\",16:\"/content/Trainframes_random16\",32:\"/content/Trainframes_random32\"},\n","    \"Val\":   {8:\"/content/Valframes_random8\",  16:\"/content/Valframes_random16\",  32:\"/content/Valframes_random32\"},\n","}\n","for split in DST_LOCAL:\n","    for n, p in DST_LOCAL[split].items():\n","        os.makedirs(p, exist_ok=True)\n","\n","\n","def list_videos_recursive(root):\n","    vids = []\n","    for r, _, fs in os.walk(root):\n","        for f in fs:\n","            if f.lower().endswith((\".mp4\",\".avi\",\".mov\",\".mkv\")):\n","                vids.append(os.path.join(r, f))\n","    return vids\n","\n","def choose_indices(total, k):\n","    if total <= k: return np.arange(total, dtype=int)\n","    return np.sort(np.random.choice(total, k, replace=False))\n","\n","def already_done(save_root, expected_frames=1):\n","    \"\"\"이미 프레임 추출된 폴더인지 확인\"\"\"\n","    if not os.path.exists(save_root):\n","        return False\n","    imgs = [f for f in os.listdir(save_root) if f.endswith(\".jpg\")]\n","    return len(imgs) >= expected_frames\n","\n","def extract_random_single_pass(video_path, save_root, k=16):\n","    \"\"\"랜덤 인덱스를 미리 정렬해 한 번만 디코딩\"\"\"\n","    # 이미 추출된 폴더면 건너뛰기\n","    if already_done(save_root, expected_frames=k//2):\n","        return \"skip\"\n","\n","    try:\n","        container = av.open(video_path)\n","        stream = container.streams.video[0]\n","        total = stream.frames\n","        os.makedirs(save_root, exist_ok=True)\n","\n","        if total and total > 0:\n","            idx = choose_indices(total, k)\n","            need = set(idx.tolist())\n","            max_need = idx[-1]\n","            saved = 0\n","            for i, frame in enumerate(container.decode(stream)):\n","                if i > max_need and saved >= len(idx):\n","                    break\n","                if i in need:\n","                    img = frame.to_ndarray(format=\"bgr24\")\n","                    cv2.imwrite(os.path.join(save_root, f\"frame_{saved}.jpg\"), img)\n","                    saved += 1\n","            container.close()\n","            return \"ok\" if saved > 0 else \"fail\"\n","\n","        # total 정보 없는 경우\n","        frames = [frm for frm in container.decode(stream)]\n","        total = len(frames)\n","        if total == 0:\n","            container.close()\n","            return \"fail\"\n","        idx = choose_indices(total, k)\n","        for j, i_sel in enumerate(idx):\n","            img = frames[i_sel].to_ndarray(format=\"bgr24\")\n","            cv2.imwrite(os.path.join(save_root, f\"frame_{j}.jpg\"), img)\n","        container.close()\n","        return \"ok\"\n","    except Exception as e:\n","        return f\"error:{e}\"\n","\n","\n","def process_split(split, n_frames_list=(8,16,32), max_workers=8):\n","    in_dir = TMP[split]\n","    files = list_videos_recursive(in_dir)\n","    total_jobs = len(files) * len(n_frames_list)\n","    print(f\"[{split}] 총 비디오 {len(files)}개, 작업 {total_jobs}개\")\n","\n","    ok = skip = fail = 0\n","    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n","        futs = []\n","        for path in files:\n","            name = os.path.splitext(os.path.basename(path))[0]\n","            for k in n_frames_list:\n","                out_dir = os.path.join(DST_LOCAL[split][k], name)\n","                futs.append(ex.submit(extract_random_single_pass, path, out_dir, k))\n","\n","        pbar = tqdm(total=len(futs),\n","                    desc=f\"{split} random (resume mode)\",\n","                    leave=False, position=0, ncols=100)\n","\n","        for fut in as_completed(futs):\n","            res = fut.result()\n","            if res == \"ok\": ok += 1\n","            elif res == \"skip\": skip += 1\n","            else: fail += 1\n","            pbar.update(1)\n","\n","        pbar.close()\n","\n","    print(f\"[{split}] 완료: 성공 {ok}, 건너뜀 {skip}, 실패 {fail}\")\n","\n","\n","process_split(\"Train\", (8,16,32), max_workers=8)\n","process_split(\"Val\",   (8,16,32), max_workers=8)\n","\n","# Drive로 동기화\n","DST_DRIVE = \"/content/drive/MyDrive/VIDEO_RETRIEVAL\"\n","for split in DST_LOCAL:\n","    for n, local_dir in DST_LOCAL[split].items():\n","        dst = os.path.join(DST_DRIVE, os.path.basename(local_dir))\n","        shutil.copytree(local_dir, dst, dirs_exist_ok=True)\n","\n","print(\" 랜덤 샘플링(단일 패스, 스킵 모드) 완료. 로컬→Drive 동기화까지 끝.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-aUs7oXRwHF","executionInfo":{"status":"ok","timestamp":1762014988550,"user_tz":-540,"elapsed":53,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"8e1719c2-2bc1-4b70-e525-44511616d996"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[Train] 총 비디오 6513개, 작업 19539개\n","[Train] 완료: 성공 0, 건너뜀 19539, 실패 0\n","[Val] 총 비디오 497개, 작업 1491개\n","[Val] 완료: 성공 0, 건너뜀 1491, 실패 0\n","랜덤 샘플링(단일 패스, 스킵 모드) 완료. 로컬→Drive 동기화까지 끝.\n"]}]},{"cell_type":"code","source":["import shutil\n","\n","folders = [\n","    \"/content/Trainframes_random8\",\n","    \"/content/Trainframes_random16\",\n","    \"/content/Trainframes_random32\"\n","]\n","\n","# 각각 zip으로 묶기\n","for f in folders:\n","    if not os.path.exists(f):\n","        print(f\" Not found: {f}\")\n","        continue\n","\n","    zip_name = f + \".zip\"\n","    print(f\" Compressing {f} → {zip_name}\")\n","    shutil.make_archive(f, 'zip', f)\n","    print(\"Done.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_bqIHFuSche","executionInfo":{"status":"ok","timestamp":1762015170333,"user_tz":-540,"elapsed":52,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"d7326842-b26d-4125-db57-a3838a81b9e6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Compressing /content/Trainframes_random8 → /content/Trainframes_random8.zip\n","Done.\n","Compressing /content/Trainframes_random16 → /content/Trainframes_random16.zip\n","Done.\n","Compressing /content/Trainframes_random32 → /content/Trainframes_random32.zip\n","Done.\n"]}]},{"cell_type":"code","source":["import os, zipfile\n","\n","base_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL/random\"\n","zip_files = [\n","    \"Trainframes_random8.zip\",\n","    \"Trainframes_random16.zip\",\n","    \"Trainframes_random32.zip\"\n","]\n","\n","for zf in zip_files:\n","    zip_path = os.path.join(base_dir, zf)\n","    extract_dir = os.path.join(base_dir, zf.replace(\".zip\", \"\"))\n","    os.makedirs(extract_dir, exist_ok=True)\n","\n","    print(f\" {zf} 압축 해제 중 → {extract_dir}\")\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_dir)\n","    print(f\" 완료: {extract_dir}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FoqKB4zpT3ko","executionInfo":{"status":"ok","timestamp":1762015543103,"user_tz":-540,"elapsed":30,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"b2771126-5ddd-46b3-e263-65816aae50aa"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Trainframes_random8.zip 압축 해제 중 → /content/drive/MyDrive/VIDEO_RETRIEVAL/random/Trainframes_random8\n","완료: /content/drive/MyDrive/VIDEO_RETRIEVAL/random/Trainframes_random8\n","\n","Trainframes_random16.zip 압축 해제 중 → /content/drive/MyDrive/VIDEO_RETRIEVAL/random/Trainframes_random16\n","완료: /content/drive/MyDrive/VIDEO_RETRIEVAL/random/Trainframes_random16\n","\n","Trainframes_random32.zip 압축 해제 중 → /content/drive/MyDrive/VIDEO_RETRIEVAL/random/Trainframes_random32\n","완료: /content/drive/MyDrive/VIDEO_RETRIEVAL/random/Trainframes_random32\n"]}]},{"cell_type":"code","source":["import torch\n","import os\n","\n","base_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL\"\n","\n","video_feats_train_path = os.path.join(base_dir, \"clip_video_embeds_train_8f.pt\")\n","video_feats_val_path   = os.path.join(base_dir, \"clip_video_embeds_val_8f.pt\")\n","text_feats_train_path  = os.path.join(base_dir, \"clip_text_embeds_train_8f.pt\")\n","text_feats_val_path    = os.path.join(base_dir, \"clip_text_embeds_val_8f.pt\")\n","\n","paths = {\n","    \"video_feats_train\": video_feats_train_path,\n","    \"video_feats_val\": video_feats_val_path,\n","    \"text_feats_train\": text_feats_train_path,\n","    \"text_feats_val\": text_feats_val_path,\n","}\n","\n","for name, path in paths.items():\n","    if not os.path.exists(path):\n","        print(f\" {name}: 파일이 존재하지 않음 ({path})\")\n","        continue\n","\n","    data = torch.load(path, map_location=\"cpu\")\n","    if isinstance(data, torch.Tensor):\n","        print(f\" {name}: {tuple(data.shape)}\")\n","    elif isinstance(data, dict):\n","        print(f\" {name}: dict 구조 ({len(data.keys())} keys)\")\n","        for k, v in data.items():\n","            if isinstance(v, torch.Tensor):\n","                print(f\"    └─ {k}: {tuple(v.shape)}\")\n","    elif isinstance(data, list):\n","        print(f\" {name}: list 구조 (길이 {len(data)})\")\n","        if isinstance(data[0], torch.Tensor):\n","            print(f\"    └─ 첫 번째 요소 크기: {tuple(data[0].shape)}\")\n","    else:\n","        print(f\" {name}: 알 수 없는 타입 {type(data)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_D7ms5STYf0","executionInfo":{"status":"ok","timestamp":1762015446279,"user_tz":-540,"elapsed":61,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"8016cb5c-7afc-4b4b-e5a2-eb5bacb4f9a0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["video_feats_train: list 구조 (길이 6513)\n","    └─ 첫 번째 요소 크기: (8, 512)\n","video_feats_val: list 구조 (길이 497)\n","    └─ 첫 번째 요소 크기: (8, 512)\n","text_feats_train: list 구조 (길이 6513)\n","    └─ 첫 번째 요소 크기: (512,)\n","text_feats_val: list 구조 (길이 497)\n","    └─ 첫 번째 요소 크기: (512,)\n"]}]},{"cell_type":"code","source":["import torch\n","import os\n","\n","base_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL\"\n","\n","video_feats_train_path = os.path.join(base_dir, \"clip_video_embeds_train_16f.pt\")\n","video_feats_val_path   = os.path.join(base_dir, \"clip_video_embeds_val_16f.pt\")\n","text_feats_train_path  = os.path.join(base_dir, \"clip_text_embeds_train_16f.pt\")\n","text_feats_val_path    = os.path.join(base_dir, \"clip_text_embeds_val_16f.pt\")\n","\n","paths = {\n","    \"video_feats_train\": video_feats_train_path,\n","    \"video_feats_val\": video_feats_val_path,\n","    \"text_feats_train\": text_feats_train_path,\n","    \"text_feats_val\": text_feats_val_path,\n","}\n","\n","for name, path in paths.items():\n","    if not os.path.exists(path):\n","        print(f\" {name}: 파일이 존재하지 않음 ({path})\")\n","        continue\n","\n","    data = torch.load(path, map_location=\"cpu\")\n","    if isinstance(data, torch.Tensor):\n","        print(f\" {name}: {tuple(data.shape)}\")\n","    elif isinstance(data, dict):\n","        print(f\" {name}: dict 구조 ({len(data.keys())} keys)\")\n","        for k, v in data.items():\n","            if isinstance(v, torch.Tensor):\n","                print(f\"    └─ {k}: {tuple(v.shape)}\")\n","    elif isinstance(data, list):\n","        print(f\" {name}: list 구조 (길이 {len(data)})\")\n","        if isinstance(data[0], torch.Tensor):\n","            print(f\"    └─ 첫 번째 요소 크기: {tuple(data[0].shape)}\")\n","    else:\n","        print(f\" {name}: 알 수 없는 타입 {type(data)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oB8_kUYTOnp","executionInfo":{"status":"ok","timestamp":1762015375925,"user_tz":-540,"elapsed":33,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"16277da3-1145-4816-86ff-d6495285384a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["video_feats_train: list 구조 (길이 6513)\n","    └─ 첫 번째 요소 크기: (16, 512)\n","video_feats_val: list 구조 (길이 497)\n","    └─ 첫 번째 요소 크기: (16, 512)\n","text_feats_train: list 구조 (길이 6513)\n","    └─ 첫 번째 요소 크기: (512,)\n","text_feats_val: list 구조 (길이 497)\n","    └─ 첫 번째 요소 크기: (512,)\n"]}]},{"cell_type":"code","source":["import torch\n","import os\n","\n","base_dir = \"/content/drive/MyDrive/VIDEO_RETRIEVAL\"\n","\n","video_feats_train_path = os.path.join(base_dir, \"clip_video_embeds_train_32f.pt\")\n","video_feats_val_path   = os.path.join(base_dir, \"clip_video_embeds_val_32f.pt\")\n","text_feats_train_path  = os.path.join(base_dir, \"clip_text_embeds_train_32f.pt\")\n","text_feats_val_path    = os.path.join(base_dir, \"clip_text_embeds_val_32f.pt\")\n","\n","paths = {\n","    \"video_feats_train\": video_feats_train_path,\n","    \"video_feats_val\": video_feats_val_path,\n","    \"text_feats_train\": text_feats_train_path,\n","    \"text_feats_val\": text_feats_val_path,\n","}\n","\n","for name, path in paths.items():\n","    if not os.path.exists(path):\n","        print(f\" {name}: 파일이 존재하지 않음 ({path})\")\n","        continue\n","\n","    data = torch.load(path, map_location=\"cpu\")\n","    if isinstance(data, torch.Tensor):\n","        print(f\" {name}: {tuple(data.shape)}\")\n","    elif isinstance(data, dict):\n","        print(f\" {name}: dict 구조 ({len(data.keys())} keys)\")\n","        for k, v in data.items():\n","            if isinstance(v, torch.Tensor):\n","                print(f\"    └─ {k}: {tuple(v.shape)}\")\n","    elif isinstance(data, list):\n","        print(f\" {name}: list 구조 (길이 {len(data)})\")\n","        if isinstance(data[0], torch.Tensor):\n","            print(f\"    └─ 첫 번째 요소 크기: {tuple(data[0].shape)}\")\n","    else:\n","        print(f\" {name}: 알 수 없는 타입 {type(data)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pjm_wZcMS8o2","executionInfo":{"status":"ok","timestamp":1762015303839,"user_tz":-540,"elapsed":24,"user":{"displayName":"아영","userId":"08504876844685829247"}},"outputId":"a74b8a5f-c0fe-46e9-e6f0-3b1d227c4702"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["video_feats_train: list 구조 (길이 6513)\n","    └─ 첫 번째 요소 크기: (32, 512)\n","video_feats_val: list 구조 (길이 497)\n","    └─ 첫 번째 요소 크기: (32, 512)\n","text_feats_train: list 구조 (길이 6513)\n","    └─ 첫 번째 요소 크기: (512,)\n","text_feats_val: list 구조 (길이 497)\n","    └─ 첫 번째 요소 크기: (512,)\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"L4","mount_file_id":"1IXXoUAfCnmipqlwWR4YxyfbZjPULFAyA","authorship_tag":"ABX9TyNXd96qs/HqZ8L3CzGkIuf/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}